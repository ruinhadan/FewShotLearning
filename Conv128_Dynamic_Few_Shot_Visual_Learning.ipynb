{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Conv128 Dynamic Few Shot Visual Learning",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruinhadan/FewShotLearning/blob/main/Conv128_Dynamic_Few_Shot_Visual_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9QA65W9WgUb"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVTuV9i0Tat-",
        "outputId": "4c49192c-f322-4c15-8e79-b159817d95c0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpktbgrXrqXA",
        "outputId": "22cf5d63-fb62-4124-b729-a21c5ba96b50"
      },
      "source": [
        "!pip install torchnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchnet\n",
            "  Downloading torchnet-0.0.4.tar.gz (23 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchnet) (1.10.0+cu111)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchnet) (1.15.0)\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[K     |████████████████████████████████| 676 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchnet) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (2.23.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (22.3.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (7.1.2)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (3.0.4)\n",
            "Building wheels for collected packages: torchnet, visdom, torchfile\n",
            "  Building wheel for torchnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchnet: filename=torchnet-0.0.4-py3-none-any.whl size=29742 sha256=8ad4d1d57ae8ddede8f5fadd47a33de35c4c79bfa7253ece813be2c4a572019b\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/17/b3/86db1d93e9dae198813aa79831b403e4844d67986cf93894b5\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=5a268f1d2a719b7a0e1993d42b6977324b5ce5cb723b00ee401caf94a3b397f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5709 sha256=1dbc8f7b069f65e7bd2cb722cc7ec0dac953dc5017a61924d194a51d821d4b7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "Successfully built torchnet visdom torchfile\n",
            "Installing collected packages: jsonpointer, websocket-client, torchfile, jsonpatch, visdom, torchnet\n",
            "Successfully installed jsonpatch-1.32 jsonpointer-2.2 torchfile-0.1.0 torchnet-0.0.4 visdom-0.1.8.9 websocket-client-1.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR__tOO34aJu"
      },
      "source": [
        "# Imports and Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2U6swyQ3JgN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import trange\n",
        "import sklearn.metrics as skm\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import torchnet as tnt\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "from PIL import ImageEnhance\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA7N_Sc_R7Ez"
      },
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.cuda.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8awocDCPt4L"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2tE23UBTW7t"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/miniImageNet_category_split_train_phase_train.pickle\", 'rb') as f:\n",
        "      ip_data = pickle.load(f, encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHv807nc5NN4"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4uRYYqA3EcM"
      },
      "source": [
        "class MiniImageNet(data.Dataset):\n",
        "  def __init__(self, data_base, data_novel=None, phase = 'train'):\n",
        "    super(MiniImageNet, self).__init__()\n",
        "\n",
        "    self.data_base = data_base\n",
        "    self.labels = data_base['labels']\n",
        "    self.phase = phase\n",
        "    self.data_novel = data_novel\n",
        "\n",
        "    if self.phase=='train':\n",
        "      self.label2ind = self.buildLabelIndex(self.labels) # {0:[0 to 599], 1:[600 to 1199]...}\n",
        "      self.labelIds = sorted(self.label2ind.keys())      # [0....63]\n",
        "      self.num_cats = len(self.labelIds)                 # 64\n",
        "      self.labelIds_base = self.labelIds                 # [0....63]\n",
        "      self.num_cats_base = len(self.labelIds_base)       # 64\n",
        "      self.data = self.data_base['data']\n",
        "\n",
        "    elif self.phase=='val' or self.phase=='test':          \n",
        "      self.data = np.concatenate([data_base['data'], data_novel['data']], axis=0)\n",
        "      self.labels = data_base['labels'] + data_novel['labels']\n",
        "\n",
        "      self.label2ind = self.buildLabelIndex(self.labels)\n",
        "      self.labelIds = sorted(self.label2ind.keys())\n",
        "      self.num_cats = len(self.labelIds)\n",
        "\n",
        "      self.labelIds_base = self.buildLabelIndex(data_base['labels']).keys()\n",
        "      self.labelIds_novel = self.buildLabelIndex(data_novel['labels']).keys()\n",
        "      self.num_cats_base = len(self.labelIds_base)\n",
        "      self.num_cats_novel = len(self.labelIds_novel)\n",
        "      intersection = set(self.labelIds_base) & set(self.labelIds_novel)\n",
        "      \n",
        "\n",
        "    mean_pix = [x/255.0 for x in [120.39586422,  115.59361427, 104.54012653]]\n",
        "    std_pix = [x/255.0 for x in [70.68188272,  68.27635443,  72.54505529]]\n",
        "    normalize = transforms.Normalize(mean=mean_pix, std=std_pix)\n",
        "\n",
        "    self.transform = transforms.Compose([\n",
        "                    transforms.RandomCrop(84, padding=8),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    lambda x: np.asarray(x),\n",
        "                    transforms.ToTensor(),\n",
        "                    normalize\n",
        "                ])\n",
        "    \n",
        "    if phase != 'train':\n",
        "      self.transform = transforms.Compose([\n",
        "                    lambda x: np.asarray(x),\n",
        "                    transforms.ToTensor(),\n",
        "                    normalize\n",
        "                ])\n",
        "      \n",
        "  def buildLabelIndex(self, labels):\n",
        "    label2inds = {}\n",
        "    for idx, label in enumerate(labels):\n",
        "        if label not in label2inds:\n",
        "            label2inds[label] = []\n",
        "        label2inds[label].append(idx)\n",
        "\n",
        "    return label2inds\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img, label = self.data[index], self.labels[index]\n",
        "    img = Image.fromarray(img)\n",
        "    img = self.transform(img)\n",
        "    return img, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "WVFCk8J3feI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FewShotDataloader():\n",
        "    def __init__(self,\n",
        "                 dataset,\n",
        "                 nKnovel=5, # number of novel categories.\n",
        "                 nKbase=-1, # number of base categories.\n",
        "                 nExemplars=5, # number of training examples per novel category.\n",
        "                 nTestNovel=15*5, # number of test examples for all the novel/base categories.\n",
        "                 nTestBase=15*5,  # 3 * nKnovel for train, 15 * nKnovel for test\n",
        "                 batch_size=1, # number of training episodes per batch.\n",
        "                 epoch_size=10, # number of batches per epoch.\n",
        "                 ):\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.nKnovel = nKnovel\n",
        "        self.nKbase = nKbase if nKbase >= 0 else 64\n",
        "        self.phase = self.dataset.phase\n",
        "        self.nExemplars = nExemplars\n",
        "        self.nTestNovel = nTestNovel\n",
        "        self.nTestBase = nTestBase\n",
        "        self.batch_size = batch_size\n",
        "        self.epoch_size = epoch_size\n",
        "        self.is_eval_mode = (self.phase=='test') or (self.phase=='val')\n",
        "\n",
        "    def sampleImageIdsFrom(self, cat_id, sample_size=5):\n",
        "\n",
        "        return random.sample(self.dataset.label2ind[cat_id], sample_size)\n",
        "\n",
        "    def sampleCategories(self, cat_set, sample_size=5):\n",
        "        \n",
        "        if cat_set=='base':\n",
        "            labelIds = self.dataset.labelIds_base\n",
        "        elif cat_set=='novel':\n",
        "            labelIds = self.dataset.labelIds_novel\n",
        "\n",
        "        assert(len(labelIds) >= sample_size)        # In case number of labels % 5 != 0        \n",
        "        return random.sample(labelIds, sample_size)\n",
        "\n",
        "    def sample_base_and_novel_categories(self, nKbase, nKnovel):\n",
        "        \n",
        "        if self.is_eval_mode:            \n",
        "            Kbase = sorted(self.sampleCategories('base', nKbase))            \n",
        "            Knovel = sorted(self.sampleCategories('novel', nKnovel))\n",
        "        else:            \n",
        "            cats_ids = self.sampleCategories('base', nKnovel+nKbase)            \n",
        "            random.shuffle(cats_ids)\n",
        "            Knovel = sorted(cats_ids[:nKnovel])       # FAKE novel\n",
        "            Kbase = sorted(cats_ids[nKnovel:])        # Remaining base\n",
        "            #print(\"Novel: \",Knovel, \"Base: \", Kbase)\n",
        "\n",
        "        return Kbase, Knovel                          # Class ids\n",
        "\n",
        "    def sample_test_examples_for_base_categories(self, Kbase, nTestBase):\n",
        "        \n",
        "        Tbase = []\n",
        "        if len(Kbase) > 0:\n",
        "            #print(len(Kbase), Kbase, nTestBase)\n",
        "            KbaseIndices = np.random.choice(np.arange(len(Kbase)), size=nTestBase, replace=True)\n",
        "            #print(KbaseIndices)\n",
        "            KbaseIndices, NumImagesPerCategory = np.unique(\n",
        "                KbaseIndices, return_counts=True)\n",
        "            \n",
        "            #print(KbaseIndices, NumImagesPerCategory)\n",
        "\n",
        "            for Kbase_idx, NumImages in zip(KbaseIndices, NumImagesPerCategory):\n",
        "                imd_ids = self.sampleImageIdsFrom(\n",
        "                    Kbase[Kbase_idx], sample_size=NumImages)\n",
        "                Tbase += [(img_id, Kbase_idx) for img_id in imd_ids]\n",
        "            \n",
        "            #print(len(Tbase))\n",
        "\n",
        "        assert(len(Tbase) == nTestBase)\n",
        "\n",
        "        return Tbase\n",
        "\n",
        "    def sample_train_and_test_examples_for_novel_categories(\n",
        "            self, Knovel, nTestNovel, nExemplars, nKbase):\n",
        "        \n",
        "\n",
        "        if len(Knovel) == 0:\n",
        "            return [], []\n",
        "\n",
        "        nKnovel = len(Knovel)\n",
        "        Tnovel = []\n",
        "        Exemplars = []\n",
        "        assert((nTestNovel % nKnovel) == 0)\n",
        "        nEvalExamplesPerClass = nTestNovel // nKnovel\n",
        "\n",
        "        for Knovel_idx in range(len(Knovel)):\n",
        "            imd_ids = self.sampleImageIdsFrom(\n",
        "                Knovel[Knovel_idx],\n",
        "                sample_size=(nEvalExamplesPerClass + nExemplars))\n",
        "\n",
        "            imds_tnovel = imd_ids[:nEvalExamplesPerClass]\n",
        "            imds_ememplars = imd_ids[nEvalExamplesPerClass:]\n",
        "\n",
        "            Tnovel += [(img_id, nKbase + Knovel_idx) for img_id in imds_tnovel]\n",
        "            Exemplars += [(img_id, nKbase + Knovel_idx) for img_id in imds_ememplars]\n",
        "        assert(len(Tnovel) == nTestNovel)\n",
        "        assert(len(Exemplars) == len(Knovel) * nExemplars)\n",
        "        random.shuffle(Exemplars)\n",
        "\n",
        "        return Tnovel, Exemplars\n",
        "\n",
        "    def sample_episode(self):\n",
        "       \n",
        "        nKnovel = self.nKnovel\n",
        "        nKbase = self.nKbase\n",
        "        nTestNovel = self.nTestNovel\n",
        "        nTestBase = self.nTestBase\n",
        "        nExemplars = self.nExemplars\n",
        "\n",
        "        Kbase, Knovel = self.sample_base_and_novel_categories(nKbase, nKnovel)\n",
        "        Tbase = self.sample_test_examples_for_base_categories(Kbase, nTestBase)\n",
        "        #print(len(Tbase))\n",
        "        Tnovel, Exemplars = self.sample_train_and_test_examples_for_novel_categories(\n",
        "            Knovel, nTestNovel, nExemplars, nKbase)\n",
        "\n",
        "       \n",
        "        Test = Tbase + Tnovel\n",
        "        random.shuffle(Test)\n",
        "        Kall = Kbase + Knovel\n",
        "\n",
        "        return Exemplars, Test, Kall, nKbase              # Exemplars - 5-way 5-shot, Test - 15 + 15, Kall - shuffle[0...63], nKbase - 59\n",
        "\n",
        "    def createExamplesTensorData(self, examples):\n",
        "        \n",
        "        images = torch.stack(\n",
        "            [self.dataset[img_idx][0] for img_idx, _ in examples], dim=0)\n",
        "        labels = torch.LongTensor([label for _, label in examples])\n",
        "        return images, labels\n",
        "\n",
        "    def get_iterator(self, epoch=0):\n",
        "        rand_seed = epoch\n",
        "        random.seed(rand_seed)\n",
        "        np.random.seed(rand_seed)\n",
        "        def load_function(iter_idx):\n",
        "            Exemplars, Test, Kall, nKbase = self.sample_episode()\n",
        "            Xt, Yt = self.createExamplesTensorData(Test)\n",
        "            Kall = torch.LongTensor(Kall)\n",
        "            if len(Exemplars) > 0:\n",
        "                Xe, Ye = self.createExamplesTensorData(Exemplars)\n",
        "                return Xe, Ye, Xt, Yt, Kall, nKbase\n",
        "            else:\n",
        "                return Xt, Yt, Kall, nKbase\n",
        "\n",
        "        tnt_dataset = tnt.dataset.ListDataset(\n",
        "            elem_list=range(self.epoch_size), load=load_function)\n",
        "        data_loader = tnt_dataset.parallel(\n",
        "            batch_size=self.batch_size,\n",
        "            # num_workers=(0 if self.is_eval_mode else self.num_workers),\n",
        "            shuffle=(False if self.is_eval_mode else True))\n",
        "\n",
        "        return data_loader\n",
        "\n",
        "    def __call__(self, epoch=0):\n",
        "        return self.get_iterator(epoch)\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.epoch_size / self.batch_size)\n"
      ],
      "metadata": {
        "id": "vMjvL5rDfc3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_ywypBD-v8r"
      },
      "source": [
        "Sample episode\n",
        "```\n",
        "torch.Size([8, 25, 3, 84, 84]) - Xe\n",
        "torch.Size([8, 25]) - Ye\n",
        "torch.Size([8, 30, 3, 84, 84]) - Xt\n",
        "torch.Size([8, 30]) - Yt\n",
        "torch.Size([8, 64]) - Kall\n",
        "torch.Size([8]) - nKbase\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_QrO6TV5SX-"
      },
      "source": [
        "# Feature Extractor & Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gscz0cmguwNN"
      },
      "source": [
        "class C128F(nn.Module):\n",
        "    def __init__(self, x_dim=3):\n",
        "        super(C128F, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            self.conv_block(x_dim, 64),\n",
        "            self.conv_block(64, 64),\n",
        "            self.conv_block(64, 128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    \n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x.view(x.size(0), -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk0KojKAHaiD"
      },
      "source": [
        "class FeatExemplarAvgBlock(nn.Module):\n",
        "    def __init__(self, nFeat):\n",
        "        super(FeatExemplarAvgBlock, self).__init__()\n",
        "\n",
        "    def forward(self, features_train, labels_train):\n",
        "        labels_train_transposed = labels_train.transpose(1,2)\n",
        "        weight_novel = torch.bmm(labels_train_transposed, features_train)\n",
        "        weight_novel = weight_novel.div(\n",
        "            labels_train_transposed.sum(dim=2, keepdim=True).expand_as(weight_novel))\n",
        "        return weight_novel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zh56rm7Q0ru"
      },
      "source": [
        "class LinearDiag(nn.Module):\n",
        "    def __init__(self, num_features, bias=False):\n",
        "        super(LinearDiag, self).__init__()\n",
        "        weight = torch.FloatTensor(num_features).fill_(1).to(device) # initialize to the identity transform\n",
        "        self.weight = nn.Parameter(weight, requires_grad=True).to(device)\n",
        "\n",
        "        if bias:\n",
        "            bias = torch.FloatTensor(num_features).fill_(0).to(device)\n",
        "            self.bias = nn.Parameter(bias, requires_grad=True).to(device)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def forward(self, X):\n",
        "        assert(X.dim()==2 and X.size(1)==self.weight.size(0))\n",
        "        out = X * self.weight.expand_as(X)\n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias.expand_as(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu_UBZAhlxys"
      },
      "source": [
        "class AttentionBasedBlock(nn.Module):\n",
        "    def __init__(self, nFeat, nK, scale_att=10.0):\n",
        "        super(AttentionBasedBlock, self).__init__()\n",
        "        self.nFeat = nFeat\n",
        "        self.queryLayer = nn.Linear(nFeat, nFeat)\n",
        "        self.queryLayer.weight.data.copy_(torch.eye(nFeat, nFeat) + torch.randn(nFeat, nFeat)*0.001)\n",
        "        self.queryLayer.bias.data.zero_()\n",
        "        scale_att = torch.FloatTensor(1).fill_(scale_att).to(device)\n",
        "        self.scale_att = nn.Parameter(scale_att, requires_grad=True).to(device)\n",
        "        wkeys = torch.FloatTensor(nK, nFeat).normal_(0.0, np.sqrt(2.0/nFeat)).to(device)\n",
        "        self.wkeys = nn.Parameter(wkeys, requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, features_train, labels_train, weight_base, Kbase):\n",
        "        batch_size, num_train_examples, num_features = features_train.size()\n",
        "        nKbase = weight_base.size(1) # [batch_size x nKbase x num_features]\n",
        "        labels_train_transposed = labels_train.transpose(1,2)\n",
        "        nKnovel = labels_train_transposed.size(1) # [batch_size x nKnovel x num_train_examples]\n",
        "\n",
        "        features_train = features_train.view(\n",
        "            batch_size*num_train_examples, num_features)\n",
        "        Qe = self.queryLayer(features_train)\n",
        "        Qe = Qe.view(batch_size, num_train_examples, self.nFeat)\n",
        "        Qe = F.normalize(Qe, p=2, dim=Qe.dim()-1, eps=1e-12)\n",
        "\n",
        "        wkeys = self.wkeys[Kbase.reshape(-1)]\n",
        "        wkeys = F.normalize(wkeys, p=2, dim=wkeys.dim()-1, eps=1e-12)\n",
        "        wkeys = wkeys.view(batch_size, nKbase, self.nFeat).transpose(1,2)\n",
        "\n",
        "        \n",
        "        AttentionCoeficients = self.scale_att * torch.bmm(Qe, wkeys)\n",
        "        AttentionCoeficients = F.softmax(AttentionCoeficients.view(batch_size*num_train_examples, nKbase))\n",
        "        AttentionCoeficients = AttentionCoeficients.view(batch_size, num_train_examples, nKbase)\n",
        "\n",
        "        \n",
        "        weight_novel = torch.bmm(AttentionCoeficients, weight_base)\n",
        "        weight_novel = torch.bmm(labels_train_transposed, weight_novel)\n",
        "        weight_novel = weight_novel.div(labels_train_transposed.sum(dim=2, keepdim=True).expand_as(weight_novel))\n",
        "\n",
        "        return weight_novel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mamZbE8J87y9"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, gen_type=None):\n",
        "    super(Classifier, self).__init__()\n",
        "\n",
        "    self.nFeat = 128*5*5 # 3200\n",
        "    self.nKall = 64\n",
        "    self.nKbase = 64 if gen_type is None else 59\n",
        "    self.nKnovel = 5\n",
        "    self.gen_type = gen_type\n",
        "\n",
        "    weight_base = torch.FloatTensor(self.nKall, self.nFeat).normal_(0.0, np.sqrt(2.0/self.nFeat)).to(device) # 64 * 3200\n",
        "    self.weight_base = nn.Parameter(weight_base, requires_grad=True).to(device)\n",
        "    bias = torch.FloatTensor(1).fill_(0).to(device)\n",
        "    self.bias = nn.Parameter(bias, requires_grad=True).to(device) # torch.tensor([0])\n",
        "    scale_cls = torch.FloatTensor(1).fill_(10.0).to(device)\n",
        "    self.scale_cls = nn.Parameter(scale_cls, requires_grad=True).to(device)\n",
        "\n",
        "    if self.gen_type=='feat_avg':\n",
        "\n",
        "      self.favgblock = FeatExemplarAvgBlock(self.nFeat)\n",
        "      self.wnLayerFavg = LinearDiag(self.nFeat, bias=True)\n",
        "    \n",
        "    elif self.gen_type=='attn':\n",
        "\n",
        "      self.favgblock = FeatExemplarAvgBlock(self.nFeat)\n",
        "      self.attblock = AttentionBasedBlock(self.nFeat, self.nKall)\n",
        "      self.wnLayerFavg = LinearDiag(self.nFeat)\n",
        "      self.wnLayerWatt = LinearDiag(self.nFeat)\n",
        "    \n",
        "  def get_classification_weights(self, features_train=None, labels_train=None, Kbase_ids=None, batch_size=8):\n",
        "    Kbase_ids = np.asarray([[i for i in range(64)] for j in range(batch_size)]) if Kbase_ids is None else Kbase_ids\n",
        "    weight_base = self.weight_base[Kbase_ids.reshape(-1)]\n",
        "    weight_base = weight_base.view(batch_size, self.nKbase, -1)\n",
        "\n",
        "    if features_train is None:\n",
        "      return weight_base\n",
        "    \n",
        "    _, num_train_examples, num_channels = features_train.size()\n",
        "    features_train = F.normalize(features_train, p=2, dim=features_train.dim()-1, eps=1e-12)\n",
        "    if self.gen_type == 'feat_avg':\n",
        "      weight_novel_avg = self.favgblock(features_train, labels_train)\n",
        "      weight_novel = self.wnLayerFavg(weight_novel_avg.view(batch_size * self.nKnovel, num_channels))\n",
        "      weight_novel = weight_novel.view(batch_size, self.nKnovel, num_channels)\n",
        "    \n",
        "    elif self.gen_type=='attn':\n",
        "      weight_novel_avg = self.favgblock(features_train, labels_train)\n",
        "      weight_novel_avg = self.wnLayerFavg(weight_novel_avg.view(batch_size * self.nKnovel, num_channels))\n",
        "      weight_base_tmp = F.normalize(weight_base, p=2, dim=weight_base.dim()-1, eps=1e-12)\n",
        "\n",
        "      weight_novel_att = self.attblock(features_train, labels_train, weight_base_tmp, Kbase_ids)\n",
        "      weight_novel_att = self.wnLayerWatt(weight_novel_att.view(batch_size * self.nKnovel, num_channels)      )\n",
        "      weight_novel = weight_novel_avg + weight_novel_att\n",
        "      weight_novel = weight_novel.view(batch_size, self.nKnovel, num_channels)\n",
        "    \n",
        "    w = torch.cat((weight_base,weight_novel),1)\n",
        "    return w   \n",
        "    \n",
        "\n",
        "  def apply_classification_weights(self, features, cls_weights):\n",
        "    features = F.normalize(features, p=2, dim=features.dim()-1, eps=1e-12) # 8 * 1 * 3200\n",
        "    cls_weights = F.normalize(cls_weights, p=2, dim=cls_weights.dim()-1, eps=1e-12) # 8 * 64 * 3200    \n",
        "    cls_scores = self.scale_cls * torch.baddbmm(self.bias.view(1, 1, 1),features, cls_weights.transpose(1,2)) # transpose to 8 * 3200 * 64\n",
        "    # 8 * [1 * 3200 x 3200 * 64 => 1 * 64 + bias]\n",
        "    return cls_scores # 8 * 1 * 64\n",
        "\n",
        "  def forward(self, features_test = None, features_train = None, labels_train=None, Kbase_ids=None, batch_size=8): \n",
        "    self.nKbase = len(Kbase_ids[0]) if Kbase_ids is not None else 64\n",
        "    cls_weights = self.get_classification_weights(features_train, labels_train, Kbase_ids, batch_size)\n",
        "    features_test = features_test.view(batch_size, 1 if features_train is None else len(features_test[0]), self.nFeat) \n",
        "    cls_scores = self.apply_classification_weights(features_test, cls_weights)\n",
        "    \n",
        "    return cls_scores\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ssrzEVA5bTQ"
      },
      "source": [
        "# Train Stage 1 - Training the feature extractor & base class classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KQzVdni6ESQ"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G26MhdMz6xo0"
      },
      "source": [
        "# input_data = MiniImageNet(ip_data)\n",
        "# dataloader = torch.utils.data.DataLoader(input_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BirmBdxasog2"
      },
      "source": [
        "input_data = MiniImageNet(ip_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A57pyYBDsm4I"
      },
      "source": [
        "dataloader = FewShotDataloader(\n",
        "    input_data, \n",
        "    nKnovel=0, \n",
        "    nKbase=64, \n",
        "    nExemplars=0, \n",
        "    nTestNovel=0, \n",
        "    nTestBase=32, \n",
        "    batch_size=8, \n",
        "    epoch_size=8000\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55I83EIB8e8J"
      },
      "source": [
        "feature_extractor = C128F().to(device)\n",
        "base_classifier = Classifier().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYGMelqzKoaC",
        "outputId": "1b5da605-a7ba-4126-cb8d-a12c9c04247c"
      },
      "source": [
        "feature_extractor.load_state_dict(torch.load(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/feat_Paper.pth\"))\n",
        "base_classifier.load_state_dict(torch.load(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_Paper.pth\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uBUyvXN6PfT"
      },
      "source": [
        "# opt_feat = optim.Adam(feature_extractor.parameters(), lr=0.1, weight_decay=1e-9)\n",
        "# opt_clas = optim.Adam(base_classifier.parameters(), lr=0.1, weight_decay=1e-9)\n",
        "# lossfn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4tADapQtY7E"
      },
      "source": [
        "opt_feat = optim.SGD(feature_extractor.parameters(), lr=0.00024, weight_decay=5e-4, momentum=0.9, nesterov=True)\n",
        "opt_clas = optim.SGD(base_classifier.parameters(), lr=0.00024, weight_decay=5e-4, momentum=0.9, nesterov=True)\n",
        "lossfn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfL9NSXS3F6x",
        "outputId": "85a2ab91-3a75-4685-a8cc-b7f13df3bdfb"
      },
      "source": [
        "feature_extractor.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "C128F(\n",
              "  (encoder): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIUC-5kx-TID",
        "outputId": "812e6b8f-e693-430d-d8da-3c7159a13312"
      },
      "source": [
        "loss_per_epoch = []\n",
        "for epoch in trange(EPOCHS):\n",
        "  sum_of_loss = 0.0\n",
        "  for Xt,Yt,Kall,nKbase in dataloader():\n",
        "    Xt = Xt.to(device).view(8*32, 3, 84, 84)\n",
        "    Yt = torch.tensor(Yt).to(device).view(8*32)\n",
        "    #print(Yt, Yt.size())\n",
        "    opt_feat.zero_grad()\n",
        "    opt_clas.zero_grad()\n",
        "    output = base_classifier(feature_extractor(Xt), batch_size=8*32)\n",
        "    loss = lossfn(output.view(8*32, 64), Yt)\n",
        "    sum_of_loss += loss.detach()\n",
        "    loss.backward()\n",
        "    opt_feat.step()\n",
        "    opt_clas.step()\n",
        "    del Xt, Yt, output\n",
        "  print(\"Epoch #\"+str(epoch)+\" Loss:\"+str(sum_of_loss.item()/1000))\n",
        "  loss_per_epoch.append(sum_of_loss/1000)\n",
        "  with open('/content/drive/MyDrive/MiniImageNet/loss_t1_Paper.txt', 'a') as f:\n",
        "    f.write('\\n'+str(sum_of_loss.item()/1000))\n",
        "    \n",
        "# plt.plot(loss_per_epoch)\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Cross Entropy Loss')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [06:49<1:01:28, 409.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #0 Loss:0.45599050903320315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [13:33<54:10, 406.25s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #1 Loss:0.4539679260253906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [20:22<47:32, 407.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #2 Loss:0.452847900390625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [27:12<40:50, 408.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #3 Loss:0.4519698486328125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [33:57<33:56, 407.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #4 Loss:0.45114337158203127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [40:42<27:05, 406.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #5 Loss:0.450466064453125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [47:26<20:17, 405.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #6 Loss:0.45155978393554685\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [54:10<13:30, 405.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #7 Loss:0.4502254943847656\n",
            "Epoch #8 Loss:0.45212460327148435\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [1:07:39<00:00, 405.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #9 Loss:0.44872283935546875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "sM18A5kk-urR",
        "outputId": "6252fc3f-acfc-4a96-d9b0-58c3883661a1"
      },
      "source": [
        "with open('/content/drive/MyDrive/MiniImageNet/loss_t1_Paper.txt', 'r') as f:\n",
        "  loss_per_epoch = f.readlines()\n",
        "\n",
        "loss_per_epoch = [float(x) for x in loss_per_epoch]\n",
        "\n",
        "plt.plot(loss_per_epoch)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cross Entropy Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2_oFWwu7J_j"
      },
      "source": [
        "# Saving models' weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOt6mawIhwkZ"
      },
      "source": [
        "torch.save(feature_extractor.state_dict(), \"/content/drive/MyDrive/MiniImageNet/MiniImagenet/feat_Paper.pth\")\n",
        "torch.save(base_classifier.state_dict(), \"/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_Paper.pth\")\n",
        "torch.save(base_classifier.state_dict()['weight_base'], \"/content/drive/MyDrive/MiniImageNet/MiniImagenet/wts_base_Paper.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUNuK3lQXjYd"
      },
      "source": [
        "# Evaluating Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42oXh4rXc_iW",
        "outputId": "b66e03db-01c6-4031-fc85-3bb942c41167"
      },
      "source": [
        "feature_extractor.load_state_dict(torch.load(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/feat_0.00001_200.pth\"))\n",
        "base_classifier.load_state_dict(torch.load(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_0.00001_200.pth\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1e8XseyW-lC"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/miniImageNet_category_split_train_phase_val.pickle\", 'rb') as f:\n",
        "      validation_data = pickle.load(f, encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd98gqCmcqVR"
      },
      "source": [
        "val_data = MiniImageNet(validation_data)\n",
        "val_dataloader = data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm6-obRg4EZV"
      },
      "source": [
        "ip_test_dataloader = data.DataLoader(input_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhrymiImn7G9",
        "outputId": "fad949fa-360b-4424-97bb-4bc99577a46d"
      },
      "source": [
        "base_classifier.load_state_dict(torch.load(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_Paper.pth\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB8Ld48yn-jc",
        "outputId": "67253975-44b0-4d59-d8d6-04a7dd4a2716"
      },
      "source": [
        "base_classifier.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('weight_base',\n",
              "              tensor([[ 0.0075,  0.0084,  0.0097,  ...,  0.0148,  0.0116, -0.0048],\n",
              "                      [ 0.0014, -0.0041, -0.0021,  ..., -0.0030,  0.0014, -0.0017],\n",
              "                      [ 0.0062,  0.0108,  0.0206,  ..., -0.0013, -0.0036, -0.0101],\n",
              "                      ...,\n",
              "                      [-0.0175, -0.0254, -0.0237,  ...,  0.0357,  0.0126,  0.0036],\n",
              "                      [ 0.0096,  0.0101,  0.0124,  ...,  0.0105,  0.0093,  0.0058],\n",
              "                      [ 0.0139,  0.0165,  0.0150,  ...,  0.0039, -0.0115, -0.0041]],\n",
              "                     device='cuda:0')),\n",
              "             ('bias', tensor([-0.0002], device='cuda:0')),\n",
              "             ('scale_cls', tensor([28.8858], device='cuda:0'))])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRT-e2ChmK7M",
        "outputId": "473a84ed-348b-49da-bd6c-213dcfb2a1cc"
      },
      "source": [
        "base_classifier_dict = base_classifier.state_dict()\n",
        "modified_state_dict = {}\n",
        "for k in base_classifier_dict.keys():\n",
        "  if k == 'scale_cls':\n",
        "    modified_state_dict[k] = torch.Tensor([28.8858])\n",
        "  else:\n",
        "    modified_state_dict[k] = base_classifier_dict[k]\n",
        "modified_state_dict = collections.OrderedDict(modified_state_dict)\n",
        "base_classifier_dict.update(modified_state_dict)\n",
        "base_classifier.load_state_dict(base_classifier_dict)\n",
        "base_classifier.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('weight_base',\n",
              "              tensor([[ 0.0075,  0.0084,  0.0097,  ...,  0.0148,  0.0116, -0.0048],\n",
              "                      [ 0.0014, -0.0041, -0.0021,  ..., -0.0030,  0.0014, -0.0017],\n",
              "                      [ 0.0062,  0.0108,  0.0206,  ..., -0.0013, -0.0036, -0.0101],\n",
              "                      ...,\n",
              "                      [-0.0175, -0.0254, -0.0237,  ...,  0.0357,  0.0126,  0.0036],\n",
              "                      [ 0.0096,  0.0101,  0.0124,  ...,  0.0105,  0.0093,  0.0058],\n",
              "                      [ 0.0139,  0.0165,  0.0150,  ...,  0.0039, -0.0115, -0.0041]],\n",
              "                     device='cuda:0')),\n",
              "             ('bias', tensor([-0.0002], device='cuda:0')),\n",
              "             ('scale_cls', tensor([28.8858], device='cuda:0'))])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npGTaByVZu6a",
        "outputId": "f2ace329-4485-4f23-821f-066818f19585"
      },
      "source": [
        "feature_extractor.eval()\n",
        "acc = 0\n",
        "cnt = 0\n",
        "for X,Y in val_dataloader:\n",
        "  X = X.to(device)\n",
        "  Y = Y.to(device)\n",
        "  output = base_classifier(feature_extractor(X))\n",
        "  pred  = torch.argmax(output.view(8,64),dim=1)\n",
        "  acc += skm.accuracy_score(Y.cpu().detach().numpy(),pred.cpu().detach().numpy())\n",
        "  cnt += 1\n",
        "\n",
        "print('Base class accuracy: ', acc/cnt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base class accuracy:  0.6886470337174563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbU7bNYQgk8M"
      },
      "source": [
        "# Train Stage 2 - Training the Few-shot classification weight generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuCrwjgr-vyx"
      },
      "source": [
        "## Instantiating classifier & loading base weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mBI4_wsixlz"
      },
      "source": [
        "input_data = MiniImageNet(ip_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY2Ay1Wa0-HR"
      },
      "source": [
        "dataloader = FewShotDataloader(\n",
        "    input_data, \n",
        "    nKnovel=5, \n",
        "    nKbase=59, \n",
        "    nExemplars=5, \n",
        "    nTestNovel=15, \n",
        "    nTestBase=15, \n",
        "    batch_size=8, \n",
        "    epoch_size=8000\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VekyfkMp5Va",
        "outputId": "9b84a3fd-3fac-449f-c418-098850b2b7a9"
      },
      "source": [
        "feature_extractor = C128F().to(device)\n",
        "feature_extractor.load_state_dict(torch.load('/content/drive/MyDrive/MiniImageNet/MiniImagenet/feat_Paper.pth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs4KFSF2G0FB"
      },
      "source": [
        "few_shot_classifier = Classifier(gen_type=\"attn\").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUUSmtyPoU5d"
      },
      "source": [
        "few_shot_classifier = Classifier(gen_type=\"feat_avg\").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oE_pHgqu434",
        "outputId": "d2ccdaa4-0223-42df-8501-bdabefb266f6"
      },
      "source": [
        "few_shot_classifier.load_state_dict(torch.load('/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_stage2_avg_Paper.pth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95ucQMrXUxbC",
        "outputId": "d66c7550-8652-4806-fcab-e58bd72e6190"
      },
      "source": [
        "few_shot_classifier_dict = few_shot_classifier.state_dict()\n",
        "base_classifier_wts = torch.load('/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_Paper.pth')\n",
        "modified_state_dict = {}\n",
        "for k in few_shot_classifier_dict.keys():\n",
        "  if k == 'weight_base':\n",
        "    modified_state_dict[k] = base_classifier_wts[k]\n",
        "  else:\n",
        "    modified_state_dict[k] = few_shot_classifier_dict[k]\n",
        "modified_state_dict = collections.OrderedDict(modified_state_dict)\n",
        "few_shot_classifier_dict.update(modified_state_dict)\n",
        "few_shot_classifier.load_state_dict(few_shot_classifier_dict)\n",
        "few_shot_classifier.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight_base',\n",
              "              tensor([[ 0.0075,  0.0084,  0.0097,  ...,  0.0148,  0.0116, -0.0048],\n",
              "                      [ 0.0014, -0.0041, -0.0021,  ..., -0.0030,  0.0014, -0.0017],\n",
              "                      [ 0.0062,  0.0108,  0.0206,  ..., -0.0013, -0.0036, -0.0101],\n",
              "                      ...,\n",
              "                      [-0.0175, -0.0254, -0.0237,  ...,  0.0357,  0.0126,  0.0036],\n",
              "                      [ 0.0096,  0.0101,  0.0124,  ...,  0.0105,  0.0093,  0.0058],\n",
              "                      [ 0.0139,  0.0165,  0.0150,  ...,  0.0039, -0.0115, -0.0041]],\n",
              "                     device='cuda:0')),\n",
              "             ('bias', tensor([0.], device='cuda:0')),\n",
              "             ('scale_cls', tensor([10.], device='cuda:0')),\n",
              "             ('wnLayerFavg.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')),\n",
              "             ('wnLayerFavg.bias',\n",
              "              tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjh7pOexeskP"
      },
      "source": [
        "opt_clas = optim.SGD(few_shot_classifier.parameters(), lr=0.1, weight_decay=5e-4, momentum=0.9, nesterov=True)\n",
        "lossfn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD1hXv0X9_jw",
        "outputId": "8f57ba7e-3bd5-48fb-8e99-7b7c09ec41c0"
      },
      "source": [
        "EPOCHS=20\n",
        "\n",
        "loss_per_epoch = []\n",
        "feature_extractor.eval()\n",
        "\n",
        "for param in feature_extractor.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "for epoch in trange(EPOCHS):\n",
        "  sum_of_loss = 0.0\n",
        "\n",
        "  for Xe,Ye,Xt,Yt,Kall,nKbase in dataloader():\n",
        "    Xe = Xe.to(device).view(8 * 25, 3, 84, 84)\n",
        "    features_train = feature_extractor(Xe)\n",
        "    features_train = features_train.resize(8,25,3200)\n",
        "    del Xe\n",
        "\n",
        "    Xt = Xt.to(device).view(8 * 30, 3, 84, 84)\n",
        "    features_test = feature_extractor(Xt)\n",
        "    features_test = features_test.resize(8,30,3200)\n",
        "    Ye = torch.tensor(Ye).to(device)\n",
        "    labels_train = []\n",
        "\n",
        "    for labels in Ye:\n",
        "      enc = OneHotEncoder()\n",
        "      labels_train.append(enc.fit_transform(labels.cpu().numpy().reshape(-1,1)).toarray())\n",
        "\n",
        "    labels_train = torch.tensor(labels_train).to(device)\n",
        "\n",
        "    opt_clas.zero_grad()\n",
        "\n",
        "    output = few_shot_classifier(features_test, features_train.float(), labels_train.float(), Kbase_ids=Kall[:,:59], batch_size=8)\n",
        "    \n",
        "    Yt = torch.tensor(Yt).to(device).view(8*30)\n",
        "    loss = lossfn(output.view(8*30,64), Yt)\n",
        "    sum_of_loss+=loss.detach()\n",
        "\n",
        "    loss.backward()    \n",
        "    opt_clas.step()\n",
        "\n",
        "  print(\"Epoch #\"+str(epoch)+\" Loss:\"+str(sum_of_loss.item()/1000))  \n",
        "  loss_per_epoch.append(sum_of_loss/1000)\n",
        "  with open('/content/drive/MyDrive/MiniImageNet/loss_t2_Paper_avg_0.1.txt', 'a') as f:\n",
        "    f.write('\\n'+str(sum_of_loss.item()/1000))\n",
        "    \n",
        "# plt.plot(loss_per_epoch)\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Cross Entropy Loss')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 1/20 [07:24<2:20:53, 444.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #0 Loss:0.9474794311523438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [14:49<2:13:21, 444.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #1 Loss:0.8817909545898438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [22:14<2:06:03, 444.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #2 Loss:0.9063026733398437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 4/20 [29:40<1:58:48, 445.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #3 Loss:0.9140669555664063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 5/20 [37:06<1:51:22, 445.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #4 Loss:0.9175047607421875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 6/20 [44:31<1:43:52, 445.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #5 Loss:0.9201671752929688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 7/20 [51:54<1:36:21, 444.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #6 Loss:0.9207140502929687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 8/20 [59:20<1:28:59, 444.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #7 Loss:0.9225994873046875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 9/20 [1:06:44<1:21:30, 444.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #8 Loss:0.9185869140625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [1:14:09<1:14:07, 444.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #9 Loss:0.9204356689453125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "VpWyEmuK8Jfc",
        "outputId": "158c223c-d110-495f-ffcb-5494e55cca49"
      },
      "source": [
        "with open('/content/drive/MyDrive/MiniImageNet/loss_attn_paper_1shot.txt', 'r') as f:\n",
        "  loss_per_epoch = f.readlines()\n",
        "\n",
        "loss_per_epoch = [float(x) for x in loss_per_epoch]\n",
        "\n",
        "plt.plot(loss_per_epoch)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cross Entropy Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1DwrV5CAUSD"
      },
      "source": [
        "## Saving model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPBs735K9RTj"
      },
      "source": [
        "torch.save(few_shot_classifier.state_dict(), \"/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_stage2_avg_Paper.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGuqpSHdL7Mh"
      },
      "source": [
        "# Evaluating on Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3mpOIjzR3bj"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/miniImageNet_category_split_val.pickle\", 'rb') as f:\n",
        "      novel_validation_data = pickle.load(f, encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fCojxuhR8st"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/miniImageNet_category_split_train_phase_val.pickle\", 'rb') as f:\n",
        "      base_validation_data = pickle.load(f, encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi3ghv-TSB5M"
      },
      "source": [
        "val_data = MiniImageNet(data_base=base_validation_data, data_novel=novel_validation_data, phase='val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnlCiIr5Smhr"
      },
      "source": [
        "# 5-way 5-shot\n",
        "\n",
        "val_dataloader_5 = FewShotDataloader(\n",
        "    val_data, \n",
        "    nKnovel=5, \n",
        "    nKbase=64,\n",
        "    nExemplars=5, \n",
        "    nTestNovel=75, \n",
        "    nTestBase=75, \n",
        "    batch_size=1, \n",
        "    epoch_size=2000\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-way 1-shot\n",
        "\n",
        "val_dataloader_1 = FewShotDataloader(\n",
        "    val_data, \n",
        "    nKnovel=5, \n",
        "    nKbase=64,\n",
        "    nExemplars=1, \n",
        "    nTestNovel=75, \n",
        "    nTestBase=75, \n",
        "    batch_size=1, \n",
        "    epoch_size=2000\n",
        ")"
      ],
      "metadata": {
        "id": "aIzwkTM42jqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Averaging"
      ],
      "metadata": {
        "id": "OAVKUZ8KjUsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-way 5-shot"
      ],
      "metadata": {
        "id": "wtkMs8csjo-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = C128F().to(device)\n",
        "feature_extractor.load_state_dict(torch.load(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/feat.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn8OAXKrru-N",
        "outputId": "2f21003e-7448-445f-80c8-01bf59920202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hJd7HFEYQI4",
        "outputId": "042c4223-8e1c-4d9c-8e17-cefba8ec0acf"
      },
      "source": [
        "few_shot_classifier = Classifier(gen_type=\"feat_avg\").to(device)\n",
        "few_shot_classifier.load_state_dict(torch.load('/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_stage2.pth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEKqI363S7bL",
        "outputId": "ab81ca51-0ef8-4c92-e5c3-bcc03c4952ce"
      },
      "source": [
        "feature_extractor.eval()\n",
        "acc_both = 0\n",
        "acc_novel = 0\n",
        "acc_base = 0\n",
        "cnt = 0\n",
        "for Xe,Ye,Xt,Yt,Kall,nKbase in val_dataloader_5():\n",
        "    Xe = Xe.to(device).view(1 * 25, 3, 84, 84)\n",
        "    features_train = feature_extractor(Xe)\n",
        "    features_train = features_train.resize(1,25,3200)\n",
        "    del Xe\n",
        "\n",
        "    Xt = Xt.to(device).view(1 * 150, 3, 84, 84)\n",
        "    features_test = feature_extractor(Xt)\n",
        "    features_test = features_test.resize(1,150,3200)\n",
        "    Ye = torch.tensor(Ye).to(device)\n",
        "    labels_train = []\n",
        "\n",
        "    for labels in Ye:\n",
        "      enc = OneHotEncoder()\n",
        "      labels_train.append(enc.fit_transform(labels.cpu().numpy().reshape(-1,1)).toarray())\n",
        "\n",
        "    labels_train = torch.tensor(labels_train).to(device)\n",
        "\n",
        "    output = few_shot_classifier(features_test, features_train.float(), labels_train.float(), Kbase_ids=None, batch_size=1)\n",
        "    \n",
        "\n",
        "    Yt = torch.tensor(Yt).to(device).view(1*150)    \n",
        "    nKbase = nKbase.to(device)\n",
        "\n",
        "    base_ids = torch.nonzero(Yt < nKbase).view(-1)\n",
        "    novel_ids = torch.nonzero(Yt >= nKbase).view(-1)\n",
        "    \n",
        "    \n",
        "    output = output.view(150,-1)\n",
        "    output_base = output[base_ids,:]\n",
        "    output_novel = output[novel_ids,:]\n",
        "    Yt_base = Yt[base_ids]\n",
        "    Yt_novel = Yt[novel_ids]\n",
        "\n",
        "    \n",
        "\n",
        "    pred_novel  = nKbase + torch.argmax(output_novel[:,nKbase:], dim=1)\n",
        "    pred_base = torch.argmax(output_base[:,:nKbase], dim=1)\n",
        "    pred_both  = torch.argmax(output,dim=1)\n",
        "\n",
        "\n",
        "    acc_both += skm.accuracy_score(Yt.cpu().detach().numpy(),pred_both.cpu().detach().numpy())\n",
        "    acc_base += skm.accuracy_score(Yt_base.cpu().detach().numpy(),pred_base.cpu().detach().numpy())\n",
        "    acc_novel += skm.accuracy_score(Yt_novel.cpu().detach().numpy(),pred_novel.cpu().detach().numpy())\n",
        "    cnt += 1\n",
        "\n",
        "print('5-way 5-shot Validation Set Accuracies [Feature Averaging]:\\n\\tBase Class Accuracy: %f\\n\\tNovel Class Accuracy: %f\\n\\tBoth Class Accuracy: %f'%((acc_base/cnt)*100.0, (acc_novel/cnt)*100.0, (acc_both/cnt)*100.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-way 5-shot Validation Set Accuracies [Feature Averaging]:\n",
            "\tBase Class Accuracy: 62.822667\n",
            "\tNovel Class Accuracy: 65.690000\n",
            "\tBoth Class Accuracy: 49.749000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-way 1-shot"
      ],
      "metadata": {
        "id": "45k7UgMcjspM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_classifier = Classifier(gen_type=\"feat_avg\").to(device)\n",
        "few_shot_classifier.load_state_dict(torch.load('/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_stage2_1shot.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbGBxEpaj-QE",
        "outputId": "10d3b237-5e1e-4a2c-9f94-db8baa435f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.eval()\n",
        "acc_both = 0\n",
        "acc_novel = 0\n",
        "acc_base = 0\n",
        "cnt = 0\n",
        "for Xe,Ye,Xt,Yt,Kall,nKbase in val_dataloader_1():\n",
        "    Xe = Xe.to(device).view(1 * 5, 3, 84, 84)\n",
        "    features_train = feature_extractor(Xe)\n",
        "    features_train = features_train.resize(1,5,3200)\n",
        "    del Xe\n",
        "\n",
        "    Xt = Xt.to(device).view(1 * 150, 3, 84, 84)\n",
        "    features_test = feature_extractor(Xt)\n",
        "    features_test = features_test.resize(1,150,3200)\n",
        "    Ye = torch.tensor(Ye).to(device)\n",
        "    labels_train = []\n",
        "\n",
        "    for labels in Ye:\n",
        "      enc = OneHotEncoder()\n",
        "      labels_train.append(enc.fit_transform(labels.cpu().numpy().reshape(-1,1)).toarray())\n",
        "\n",
        "    labels_train = torch.tensor(labels_train).to(device)\n",
        "\n",
        "    output = few_shot_classifier(features_test, features_train.float(), labels_train.float(), Kbase_ids=None, batch_size=1)\n",
        "    \n",
        "\n",
        "    Yt = torch.tensor(Yt).to(device).view(1*150)    \n",
        "    nKbase = nKbase.to(device)\n",
        "\n",
        "    base_ids = torch.nonzero(Yt < nKbase).view(-1)\n",
        "    novel_ids = torch.nonzero(Yt >= nKbase).view(-1)\n",
        "    \n",
        "    \n",
        "    output = output.view(150,-1)\n",
        "    output_base = output[base_ids,:]\n",
        "    output_novel = output[novel_ids,:]\n",
        "    Yt_base = Yt[base_ids]\n",
        "    Yt_novel = Yt[novel_ids]\n",
        "\n",
        "    \n",
        "\n",
        "    pred_novel  = nKbase + torch.argmax(output_novel[:,nKbase:], dim=1)\n",
        "    pred_base = torch.argmax(output_base[:,:nKbase], dim=1)\n",
        "    pred_both  = torch.argmax(output,dim=1)\n",
        "\n",
        "\n",
        "    acc_both += skm.accuracy_score(Yt.cpu().detach().numpy(),pred_both.cpu().detach().numpy())\n",
        "    acc_base += skm.accuracy_score(Yt_base.cpu().detach().numpy(),pred_base.cpu().detach().numpy())\n",
        "    acc_novel += skm.accuracy_score(Yt_novel.cpu().detach().numpy(),pred_novel.cpu().detach().numpy())\n",
        "    cnt += 1\n",
        "\n",
        "print('5-way 1-shot Accuracies on Validation Set [Feature Averaging]:\\n\\tBase Class Accuracy: %f\\n\\tNovel Class Accuracy: %f\\n\\tBoth Class Accuracy: %f'%((acc_base/cnt)*100.0, (acc_novel/cnt)*100.0, (acc_both/cnt)*100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYVbfnJ-2sy6",
        "outputId": "2907794f-a6c1-42ae-af1f-57d0de0c7eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-way 1-shot Accuracies on Validation Set [Feature Averaging]:\n",
            "\tBase Class Accuracy: 62.427333\n",
            "\tNovel Class Accuracy: 48.004667\n",
            "\tBoth Class Accuracy: 41.031000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "r54sZg95jgAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-way 5-shot"
      ],
      "metadata": {
        "id": "CKggIm3jjvHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = C128F().to(device)\n",
        "feature_extractor.load_state_dict(torch.load(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/feat_Paper.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxhiDgElvkfb",
        "outputId": "a5f82452-7016-4341-b792-b23f73a94ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_classifier = Classifier(gen_type=\"attn\").to(device)\n",
        "few_shot_classifier.load_state_dict(torch.load('/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_stage2_attn_Paper_60.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y53mujT8jiQm",
        "outputId": "cc5ab8d3-2398-4da3-9bad-c58b81f08586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.eval()\n",
        "acc_both = 0\n",
        "acc_novel = 0\n",
        "acc_base = 0\n",
        "cnt = 0\n",
        "for Xe,Ye,Xt,Yt,Kall,nKbase in val_dataloader_5():\n",
        "    Xe = Xe.to(device).view(1 * 25, 3, 84, 84)\n",
        "    features_train = feature_extractor(Xe)\n",
        "    features_train = features_train.resize(1,25,3200)\n",
        "    del Xe\n",
        "\n",
        "    Xt = Xt.to(device).view(1 * 150, 3, 84, 84)\n",
        "    features_test = feature_extractor(Xt)\n",
        "    features_test = features_test.resize(1,150,3200)\n",
        "    Ye = torch.tensor(Ye).to(device)\n",
        "    labels_train = []\n",
        "\n",
        "    for labels in Ye:\n",
        "      enc = OneHotEncoder()\n",
        "      labels_train.append(enc.fit_transform(labels.cpu().numpy().reshape(-1,1)).toarray())\n",
        "\n",
        "    labels_train = torch.tensor(labels_train).to(device)\n",
        "\n",
        "    output = few_shot_classifier(features_test, features_train.float(), labels_train.float(), Kbase_ids=None, batch_size=1)\n",
        "    \n",
        "\n",
        "    Yt = torch.tensor(Yt).to(device).view(1*150)    \n",
        "    nKbase = nKbase.to(device)\n",
        "\n",
        "    base_ids = torch.nonzero(Yt < nKbase).view(-1)\n",
        "    novel_ids = torch.nonzero(Yt >= nKbase).view(-1)\n",
        "    \n",
        "    \n",
        "    output = output.view(150,-1)\n",
        "    output_base = output[base_ids,:]\n",
        "    output_novel = output[novel_ids,:]\n",
        "    Yt_base = Yt[base_ids]\n",
        "    Yt_novel = Yt[novel_ids]\n",
        "\n",
        "    \n",
        "\n",
        "    pred_novel  = nKbase + torch.argmax(output_novel[:,nKbase:], dim=1)\n",
        "    pred_base = torch.argmax(output_base[:,:nKbase], dim=1)\n",
        "    pred_both  = torch.argmax(output,dim=1)\n",
        "\n",
        "\n",
        "    acc_both += skm.accuracy_score(Yt.cpu().detach().numpy(),pred_both.cpu().detach().numpy())\n",
        "    acc_base += skm.accuracy_score(Yt_base.cpu().detach().numpy(),pred_base.cpu().detach().numpy())\n",
        "    acc_novel += skm.accuracy_score(Yt_novel.cpu().detach().numpy(),pred_novel.cpu().detach().numpy())\n",
        "    cnt += 1\n",
        "\n",
        "print('5-way 5-shot Validation Set Accuracies [Attention]:\\n\\tBase Class Accuracy: %f\\n\\tNovel Class Accuracy: %f\\n\\tBoth Class Accuracy: %f'%((acc_base/cnt)*100.0, (acc_novel/cnt)*100.0, (acc_both/cnt)*100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36Gg4Oq4j2fe",
        "outputId": "52af17d7-71db-4a2c-f8a5-c43e8263bd8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-way 5-shot Validation Set Accuracies [Attention]:\n",
            "\tBase Class Accuracy: 70.367333\n",
            "\tNovel Class Accuracy: 73.127333\n",
            "\tBoth Class Accuracy: 58.022667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-way 1-shot"
      ],
      "metadata": {
        "id": "KwWdG1eWj61j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_classifier = Classifier(gen_type=\"attn\").to(device)\n",
        "few_shot_classifier.load_state_dict(torch.load('/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_stage2_1shot_paper_attn_60.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xnkDjDbj8pG",
        "outputId": "59d4f7c1-b13f-42de-b63e-72bd16342f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.eval()\n",
        "acc_both = 0\n",
        "acc_novel = 0\n",
        "acc_base = 0\n",
        "cnt = 0\n",
        "for Xe,Ye,Xt,Yt,Kall,nKbase in val_dataloader_1():\n",
        "    Xe = Xe.to(device).view(1 * 5, 3, 84, 84)\n",
        "    features_train = feature_extractor(Xe)\n",
        "    features_train = features_train.resize(1,5,3200)\n",
        "    del Xe\n",
        "\n",
        "    Xt = Xt.to(device).view(1 * 150, 3, 84, 84)\n",
        "    features_test = feature_extractor(Xt)\n",
        "    features_test = features_test.resize(1,150,3200)\n",
        "    Ye = torch.tensor(Ye).to(device)\n",
        "    labels_train = []\n",
        "\n",
        "    for labels in Ye:\n",
        "      enc = OneHotEncoder()\n",
        "      labels_train.append(enc.fit_transform(labels.cpu().numpy().reshape(-1,1)).toarray())\n",
        "\n",
        "    labels_train = torch.tensor(labels_train).to(device)\n",
        "\n",
        "    output = few_shot_classifier(features_test, features_train.float(), labels_train.float(), Kbase_ids=None, batch_size=1)\n",
        "    \n",
        "\n",
        "    Yt = torch.tensor(Yt).to(device).view(1*150)    \n",
        "    nKbase = nKbase.to(device)\n",
        "\n",
        "    base_ids = torch.nonzero(Yt < nKbase).view(-1)\n",
        "    novel_ids = torch.nonzero(Yt >= nKbase).view(-1)\n",
        "    \n",
        "    \n",
        "    output = output.view(150,-1)\n",
        "    output_base = output[base_ids,:]\n",
        "    output_novel = output[novel_ids,:]\n",
        "    Yt_base = Yt[base_ids]\n",
        "    Yt_novel = Yt[novel_ids]\n",
        "\n",
        "    \n",
        "\n",
        "    pred_novel  = nKbase + torch.argmax(output_novel[:,nKbase:], dim=1)\n",
        "    pred_base = torch.argmax(output_base[:,:nKbase], dim=1)\n",
        "    pred_both  = torch.argmax(output,dim=1)\n",
        "\n",
        "\n",
        "    acc_both += skm.accuracy_score(Yt.cpu().detach().numpy(),pred_both.cpu().detach().numpy())\n",
        "    acc_base += skm.accuracy_score(Yt_base.cpu().detach().numpy(),pred_base.cpu().detach().numpy())\n",
        "    acc_novel += skm.accuracy_score(Yt_novel.cpu().detach().numpy(),pred_novel.cpu().detach().numpy())\n",
        "    cnt += 1\n",
        "\n",
        "print('5-way 1-shot Accuracies on Validation Set [Attention]:\\n\\tBase Class Accuracy: %f\\n\\tNovel Class Accuracy: %f\\n\\tBoth Class Accuracy: %f'%((acc_base/cnt)*100.0, (acc_novel/cnt)*100.0, (acc_both/cnt)*100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_5hhTbvkCw8",
        "outputId": "9f3c41e5-664e-4166-fe56-b954531054d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-way 1-shot Accuracies on Validation Set [Attention]:\n",
            "\tBase Class Accuracy: 70.406000\n",
            "\tNovel Class Accuracy: 56.845333\n",
            "\tBoth Class Accuracy: 48.548333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsgqlRVgpcWy"
      },
      "source": [
        "# Evaluating on Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arc54COcpgFn"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/miniImageNet_category_split_test.pickle\", 'rb') as f:\n",
        "      novel_test_data = pickle.load(f, encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxBcsFY-pgFn"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/miniImageNet_category_split_train_phase_test.pickle\", 'rb') as f:\n",
        "      base_test_data = pickle.load(f, encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph8Z7PKOpgFo"
      },
      "source": [
        "test_data = MiniImageNet(data_base=base_test_data, data_novel=novel_test_data, phase='val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mFsx21wpgFo"
      },
      "source": [
        "# 5-way 5-shot\n",
        "\n",
        "test_dataloader_5 = FewShotDataloader(\n",
        "    test_data, \n",
        "    nKnovel=5, \n",
        "    nKbase=64,\n",
        "    nExemplars=5, \n",
        "    nTestNovel=75, \n",
        "    nTestBase=75, \n",
        "    batch_size=1, \n",
        "    epoch_size=600\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-way 1-shot\n",
        "\n",
        "test_dataloader_1 = FewShotDataloader(\n",
        "    test_data, \n",
        "    nKnovel=5, \n",
        "    nKbase=64,\n",
        "    nExemplars=1, \n",
        "    nTestNovel=75, \n",
        "    nTestBase=75, \n",
        "    batch_size=1, \n",
        "    epoch_size=600\n",
        ")"
      ],
      "metadata": {
        "id": "InbSg_GD3Esa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Averaging"
      ],
      "metadata": {
        "id": "RjWwygT_kQ8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-way 5-shot"
      ],
      "metadata": {
        "id": "HptC07jakTJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = C128F().to(device)\n",
        "feature_extractor.load_state_dict(torch.load(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/feat.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAg18j72wO_c",
        "outputId": "f89fde61-ee93-4ff6-ce17-eb9088f8e2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpxW6twgpgFo",
        "outputId": "fc7d18ad-0edf-4360-f90f-5bd32a547d80"
      },
      "source": [
        "few_shot_classifier = Classifier(gen_type=\"feat_avg\").to(device)\n",
        "few_shot_classifier.load_state_dict(torch.load('/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_stage2.pth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cCc98_ApgFp",
        "outputId": "b9ae7ad5-cc3c-4eda-83f1-f3c3c10dbe85"
      },
      "source": [
        "feature_extractor.eval()\n",
        "acc_both = 0\n",
        "acc_novel = 0\n",
        "acc_base = 0\n",
        "cnt = 0\n",
        "for Xe,Ye,Xt,Yt,Kall,nKbase in test_dataloader_5():\n",
        "    Xe = Xe.to(device).view(1 * 25, 3, 84, 84)\n",
        "    features_train = feature_extractor(Xe)\n",
        "    features_train = features_train.resize(1,25,3200)\n",
        "    del Xe\n",
        "\n",
        "    Xt = Xt.to(device).view(1 * 150, 3, 84, 84)\n",
        "    features_test = feature_extractor(Xt)\n",
        "    features_test = features_test.resize(1,150,3200)\n",
        "    Ye = torch.tensor(Ye).to(device)\n",
        "    labels_train = []\n",
        "\n",
        "    for labels in Ye:\n",
        "      enc = OneHotEncoder()\n",
        "      labels_train.append(enc.fit_transform(labels.cpu().numpy().reshape(-1,1)).toarray())\n",
        "\n",
        "    labels_train = torch.tensor(labels_train).to(device)\n",
        "\n",
        "    output = few_shot_classifier(features_test, features_train.float(), labels_train.float(), Kbase_ids=None, batch_size=1)\n",
        "    \n",
        "\n",
        "    Yt = torch.tensor(Yt).to(device).view(1*150)    \n",
        "    nKbase = nKbase.to(device)\n",
        "\n",
        "    base_ids = torch.nonzero(Yt < nKbase).view(-1)\n",
        "    novel_ids = torch.nonzero(Yt >= nKbase).view(-1)\n",
        "    \n",
        "    \n",
        "    output = output.view(150,-1)\n",
        "    output_base = output[base_ids,:]\n",
        "    output_novel = output[novel_ids,:]\n",
        "    Yt_base = Yt[base_ids]\n",
        "    Yt_novel = Yt[novel_ids]\n",
        "\n",
        "    \n",
        "\n",
        "    pred_novel  = nKbase + torch.argmax(output_novel[:,nKbase:], dim=1)\n",
        "    pred_base = torch.argmax(output_base[:,:nKbase], dim=1)\n",
        "    pred_both  = torch.argmax(output,dim=1)\n",
        "\n",
        "\n",
        "    acc_both += skm.accuracy_score(Yt.cpu().detach().numpy(),pred_both.cpu().detach().numpy())\n",
        "    acc_base += skm.accuracy_score(Yt_base.cpu().detach().numpy(),pred_base.cpu().detach().numpy())\n",
        "    acc_novel += skm.accuracy_score(Yt_novel.cpu().detach().numpy(),pred_novel.cpu().detach().numpy())\n",
        "    cnt += 1\n",
        "\n",
        "print('5-way 5-shot Test Set Accuracies [Feature Averaging]:\\n\\tBase Class Accuracy: %f\\n\\tNovel Class Accuracy: %f\\n\\tBoth Class Accuracy: %f'%((acc_base/cnt)*100.0, (acc_novel/cnt)*100.0, (acc_both/cnt)*100.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-way 5-shot Test Set Accuracies [Feature Averaging]:\n",
            "\tBase Class Accuracy: 61.564444\n",
            "\tNovel Class Accuracy: 66.744444\n",
            "\tBoth Class Accuracy: 50.903333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-way 1-shot"
      ],
      "metadata": {
        "id": "Yp2aj-IHkzJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_classifier = Classifier(gen_type=\"feat_avg\").to(device)\n",
        "few_shot_classifier.load_state_dict(torch.load('/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_stage2_1shot.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpCBeuXvk2V0",
        "outputId": "d4ff78a7-c6c5-4e2e-877f-92be66a43ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.eval()\n",
        "acc_both = 0\n",
        "acc_novel = 0\n",
        "acc_base = 0\n",
        "cnt = 0\n",
        "for Xe,Ye,Xt,Yt,Kall,nKbase in test_dataloader_1():\n",
        "    Xe = Xe.to(device).view(1 * 5, 3, 84, 84)\n",
        "    features_train = feature_extractor(Xe)\n",
        "    features_train = features_train.resize(1,5,3200)\n",
        "    del Xe\n",
        "\n",
        "    Xt = Xt.to(device).view(1 * 150, 3, 84, 84)\n",
        "    features_test = feature_extractor(Xt)\n",
        "    features_test = features_test.resize(1,150,3200)\n",
        "    Ye = torch.tensor(Ye).to(device)\n",
        "    labels_train = []\n",
        "\n",
        "    for labels in Ye:\n",
        "      enc = OneHotEncoder()\n",
        "      labels_train.append(enc.fit_transform(labels.cpu().numpy().reshape(-1,1)).toarray())\n",
        "\n",
        "    labels_train = torch.tensor(labels_train).to(device)\n",
        "\n",
        "    output = few_shot_classifier(features_test, features_train.float(), labels_train.float(), Kbase_ids=None, batch_size=1)\n",
        "    \n",
        "\n",
        "    Yt = torch.tensor(Yt).to(device).view(1*150)    \n",
        "    nKbase = nKbase.to(device)\n",
        "\n",
        "    base_ids = torch.nonzero(Yt < nKbase).view(-1)\n",
        "    novel_ids = torch.nonzero(Yt >= nKbase).view(-1)\n",
        "    \n",
        "    \n",
        "    output = output.view(150,-1)\n",
        "    output_base = output[base_ids,:]\n",
        "    output_novel = output[novel_ids,:]\n",
        "    Yt_base = Yt[base_ids]\n",
        "    Yt_novel = Yt[novel_ids]\n",
        "\n",
        "    \n",
        "\n",
        "    pred_novel  = nKbase + torch.argmax(output_novel[:,nKbase:], dim=1)\n",
        "    pred_base = torch.argmax(output_base[:,:nKbase], dim=1)\n",
        "    pred_both  = torch.argmax(output,dim=1)\n",
        "\n",
        "\n",
        "    acc_both += skm.accuracy_score(Yt.cpu().detach().numpy(),pred_both.cpu().detach().numpy())\n",
        "    acc_base += skm.accuracy_score(Yt_base.cpu().detach().numpy(),pred_base.cpu().detach().numpy())\n",
        "    acc_novel += skm.accuracy_score(Yt_novel.cpu().detach().numpy(),pred_novel.cpu().detach().numpy())\n",
        "    cnt += 1\n",
        "\n",
        "print('5-way 1-shot Test Set Accuracies [Feature Averaging]:\\n\\tBase Class Accuracy: %f\\n\\tNovel Class Accuracy: %f\\n\\tBoth Class Accuracy: %f'%((acc_base/cnt)*100.0, (acc_novel/cnt)*100.0, (acc_both/cnt)*100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOBoCxF33ISz",
        "outputId": "ca77891a-7b4e-4fd5-b2e0-77555478cfe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-way 1-shot Test Set Accuracies [Feature Averaging]:\n",
            "\tBase Class Accuracy: 61.431111\n",
            "\tNovel Class Accuracy: 50.160000\n",
            "\tBoth Class Accuracy: 42.183333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "0IYHLkqwlAu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-way 5-shot"
      ],
      "metadata": {
        "id": "lIPJ6M5-lFid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = C128F().to(device)\n",
        "feature_extractor.load_state_dict(torch.load(\"/content/drive/MyDrive/MiniImageNet/MiniImagenet/feat_Paper.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1mzoM7nwR-q",
        "outputId": "ec923e0c-6a82-4ec8-86bf-4029244a25b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_classifier = Classifier(gen_type=\"attn\").to(device)\n",
        "few_shot_classifier.load_state_dict(torch.load('/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_stage2_attn_Paper_60.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2k6Ug53lBv8",
        "outputId": "c2f5b6cf-f29b-4924-e01d-b086344ce14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.eval()\n",
        "acc_both = 0\n",
        "acc_novel = 0\n",
        "acc_base = 0\n",
        "cnt = 0\n",
        "for Xe,Ye,Xt,Yt,Kall,nKbase in test_dataloader_5():\n",
        "    Xe = Xe.to(device).view(1 * 25, 3, 84, 84)\n",
        "    features_train = feature_extractor(Xe)\n",
        "    features_train = features_train.resize(1,25,3200)\n",
        "    del Xe\n",
        "\n",
        "    Xt = Xt.to(device).view(1 * 150, 3, 84, 84)\n",
        "    features_test = feature_extractor(Xt)\n",
        "    features_test = features_test.resize(1,150,3200)\n",
        "    Ye = torch.tensor(Ye).to(device)\n",
        "    labels_train = []\n",
        "\n",
        "    for labels in Ye:\n",
        "      enc = OneHotEncoder()\n",
        "      labels_train.append(enc.fit_transform(labels.cpu().numpy().reshape(-1,1)).toarray())\n",
        "\n",
        "    labels_train = torch.tensor(labels_train).to(device)\n",
        "\n",
        "    output = few_shot_classifier(features_test, features_train.float(), labels_train.float(), Kbase_ids=None, batch_size=1)\n",
        "    \n",
        "\n",
        "    Yt = torch.tensor(Yt).to(device).view(1*150)    \n",
        "    nKbase = nKbase.to(device)\n",
        "\n",
        "    base_ids = torch.nonzero(Yt < nKbase).view(-1)\n",
        "    novel_ids = torch.nonzero(Yt >= nKbase).view(-1)\n",
        "    \n",
        "    \n",
        "    output = output.view(150,-1)\n",
        "    output_base = output[base_ids,:]\n",
        "    output_novel = output[novel_ids,:]\n",
        "    Yt_base = Yt[base_ids]\n",
        "    Yt_novel = Yt[novel_ids]\n",
        "\n",
        "    \n",
        "\n",
        "    pred_novel  = nKbase + torch.argmax(output_novel[:,nKbase:], dim=1)\n",
        "    pred_base = torch.argmax(output_base[:,:nKbase], dim=1)\n",
        "    pred_both  = torch.argmax(output,dim=1)\n",
        "\n",
        "\n",
        "    acc_both += skm.accuracy_score(Yt.cpu().detach().numpy(),pred_both.cpu().detach().numpy())\n",
        "    acc_base += skm.accuracy_score(Yt_base.cpu().detach().numpy(),pred_base.cpu().detach().numpy())\n",
        "    acc_novel += skm.accuracy_score(Yt_novel.cpu().detach().numpy(),pred_novel.cpu().detach().numpy())\n",
        "    cnt += 1\n",
        "\n",
        "print('5-way 5-shot Test Set Accuracies [Attention]:\\n\\tBase Class Accuracy: %f\\n\\tNovel Class Accuracy: %f\\n\\tBoth Class Accuracy: %f'%((acc_base/cnt)*100.0, (acc_novel/cnt)*100.0, (acc_both/cnt)*100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78Kh1Af-lLQj",
        "outputId": "33ef084f-a1dd-4e25-d604-65c32440ac75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-way 5-shot Test Set Accuracies [Attention]:\n",
            "\tBase Class Accuracy: 70.128889\n",
            "\tNovel Class Accuracy: 71.248889\n",
            "\tBoth Class Accuracy: 56.648889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-way 1-shot"
      ],
      "metadata": {
        "id": "ibrhBGSxlQDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_classifier = Classifier(gen_type=\"attn\").to(device)\n",
        "few_shot_classifier.load_state_dict(torch.load('/content/drive/MyDrive/MiniImageNet/MiniImagenet/clas_stage2_1shot_paper_attn_60.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky1dyMu-lRcA",
        "outputId": "e2be3b27-a594-42ab-9975-471c113f9d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.eval()\n",
        "acc_both = 0\n",
        "acc_novel = 0\n",
        "acc_base = 0\n",
        "cnt = 0\n",
        "for Xe,Ye,Xt,Yt,Kall,nKbase in test_dataloader_1():\n",
        "    Xe = Xe.to(device).view(1 * 5, 3, 84, 84)\n",
        "    features_train = feature_extractor(Xe)\n",
        "    features_train = features_train.resize(1,5,3200)\n",
        "    del Xe\n",
        "\n",
        "    Xt = Xt.to(device).view(1 * 150, 3, 84, 84)\n",
        "    features_test = feature_extractor(Xt)\n",
        "    features_test = features_test.resize(1,150,3200)\n",
        "    Ye = torch.tensor(Ye).to(device)\n",
        "    labels_train = []\n",
        "\n",
        "    for labels in Ye:\n",
        "      enc = OneHotEncoder()\n",
        "      labels_train.append(enc.fit_transform(labels.cpu().numpy().reshape(-1,1)).toarray())\n",
        "\n",
        "    labels_train = torch.tensor(labels_train).to(device)\n",
        "\n",
        "    output = few_shot_classifier(features_test, features_train.float(), labels_train.float(), Kbase_ids=None, batch_size=1)\n",
        "    \n",
        "\n",
        "    Yt = torch.tensor(Yt).to(device).view(1*150)    \n",
        "    nKbase = nKbase.to(device)\n",
        "\n",
        "    base_ids = torch.nonzero(Yt < nKbase).view(-1)\n",
        "    novel_ids = torch.nonzero(Yt >= nKbase).view(-1)\n",
        "    \n",
        "    \n",
        "    output = output.view(150,-1)\n",
        "    output_base = output[base_ids,:]\n",
        "    output_novel = output[novel_ids,:]\n",
        "    Yt_base = Yt[base_ids]\n",
        "    Yt_novel = Yt[novel_ids]\n",
        "\n",
        "    \n",
        "\n",
        "    pred_novel  = nKbase + torch.argmax(output_novel[:,nKbase:], dim=1)\n",
        "    pred_base = torch.argmax(output_base[:,:nKbase], dim=1)\n",
        "    pred_both  = torch.argmax(output,dim=1)\n",
        "\n",
        "\n",
        "    acc_both += skm.accuracy_score(Yt.cpu().detach().numpy(),pred_both.cpu().detach().numpy())\n",
        "    acc_base += skm.accuracy_score(Yt_base.cpu().detach().numpy(),pred_base.cpu().detach().numpy())\n",
        "    acc_novel += skm.accuracy_score(Yt_novel.cpu().detach().numpy(),pred_novel.cpu().detach().numpy())\n",
        "    cnt += 1\n",
        "\n",
        "print('5-way 1-shot Test Set Accuracies [Attention]:\\n\\tBase Class Accuracy: %f\\n\\tNovel Class Accuracy: %f\\n\\tBoth Class Accuracy: %f'%((acc_base/cnt)*100.0, (acc_novel/cnt)*100.0, (acc_both/cnt)*100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKxabKQalVJl",
        "outputId": "9bf196c4-ffa8-4c83-9f24-8c604fbcbd42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-way 1-shot Test Set Accuracies [Attention]:\n",
            "\tBase Class Accuracy: 70.004444\n",
            "\tNovel Class Accuracy: 54.824444\n",
            "\tBoth Class Accuracy: 47.465556\n"
          ]
        }
      ]
    }
  ]
}